---
title: "Module-22"
output: html_document
---

```{r}
source("https://bioconductor.org/biocLite.R")  # loads a of functions that allows us to access and install Bioconductor packages in addition to CRAN packages
biocLite("Rgraphviz", suppressUpdates = TRUE)  # installs the {Rgraphviz} package and suppresses updating of other packages from Bioconductor
```
#Text Mining
```{r}
library(tm)
```
```{r}
library(SnowballC)
```

##Creating a Corpus from a Folder of Text Files
```{r}
path <- "C:/Users/Andy/Desktop/texts"
dirCorpus <- Corpus(DirSource(path))  # read in text documents... within each document, content is a vector of character strings
summary(dirCorpus)
```
```{r}
head(dirCorpus[[1]]$content)  # show the start of document 1
```
#Creating a Corpus from a Single Text File with Multiple Documents
```{r}
library(curl)
library(stringr)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/complete_jane_austen.txt")
f <- scan(file = f, what = "character", sep = "")  # read in text document... this function, separates every word
doc <- paste(f, collapse = " ")  # collapses the complete text by spaces... creates a single long character string
docs <- str_split(doc, "THE END")[[1]]  # splits the doc into a vector of docs
fileCorpus <- Corpus(VectorSource(docs))  # converts the split doc into a corpus of documents; within each document, content is a single character string
summary(fileCorpus)
```
```{r}
# to remove empty docs from corpus
for (i in 1:length(fileCorpus)) {
    if (fileCorpus[[i]]$content == "") {
        fileCorpus[[i]] <- NULL
    }
}
titles <- c("Persuasion", "Northanger Abbey", "Mansfield Park", "Emma", "Love and Friendship and Other Early Works", 
    "Pride and Prejudice", "Sense and Sensibility")
for (i in 1:length(fileCorpus)) {
    # this loop assigns titles to documents
    fileCorpus[[i]]$meta$id <- titles[i]
}
fileCorpus[[1]]$meta  # show the metadata for document 1
```
```{r}
head(fileCorpus[[1]]$content)  # show the start of document 1
```
#Creating a Corpus from a Data Frame or Vector
```{r}
library(curl)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/potustweets.csv")
f <- read.csv(f, header = TRUE, sep = ",")
tweetCorpus <- Corpus(VectorSource(f$text))  # each document is the text of a tweet
# summary(tweetCorpus) # NOTE: THIS LINE NOT RUN TO AVOID COPIOUS OUTPUT
# inspect(tweetCorpus) # NOTE: THIS LINE NOT RUN TO AVOID COPIOUS OUTPUT to
# remove empty docs from corpus
for (i in 1:length(tweetCorpus)) {
    if (tweetCorpus[[i]]$content == "") {
        tweetCorpus[[i]] <- NULL
    }
}
tweetCorpus[[1]]$meta  # show the metadata for document 
```
```{r}
head(tweetCorpus[[1]]$content)  # show the start of document 1
```
#Pre-Processing
```{r}
removeURLs <- content_transformer(function(x) gsub("http[^[:space:]]*", "", 
    x))
tweetCorpus <- tm_map(tweetCorpus, removeURLs)
```

```{r}
replace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
dirCorpus <- tm_map(dirCorpus, replace, "[!@#$%^&*|\\]")  # replaces odd characters; double backslash is really escape character '\' plus '\'
fileCorpus <- tm_map(fileCorpus, replace, "[!@#$%^&*|\\]")  # replaces odd characters
tweetCorpus <- tm_map(tweetCorpus, replace, "[!@#$%^&*|\\]")  # replaces odd characters
```

```{r}
dirCorpus <- tm_map(dirCorpus, content_transformer(tolower))  # we wrap the function `tolower` in `content_transformer()` because it is not a function built into the {tm} package
fileCorpus <- tm_map(fileCorpus, content_transformer(tolower))
tweetCorpus <- tm_map(tweetCorpus, content_transformer(tolower))
```

```{r}
dirCorpus <- tm_map(dirCorpus, removePunctuation)
fileCorpus <- tm_map(fileCorpus, removePunctuation)
tweetCorpus <- tm_map(tweetCorpus, removePunctuation)
```

```{r}
dirCorpus <- tm_map(dirCorpus, removeNumbers)
fileCorpus <- tm_map(fileCorpus, removeNumbers)
tweetCorpus <- tm_map(tweetCorpus, removeNumbers)
```

```{r}
stopwords("english")  # built in list of stopwords
```
```{r, eval=FALSE}
mystopwords <- c(stopwords("english"))  # we can add or remove words from this list to this
dirCorpus <- tm_map(dirCorpus, removeWords, mystopwords)
fileCorpus <- tm_map(fileCorpus, removeWords, mystopwords)
tweetCorpus <- tm_map(tweetCorpus, removeWords, mystopwords)
```
don't know why I got this error
```{r, eval=FALSE}
toCut <- c("email", "Austin")
dirCorpus <- tm_map(dirCorpus, removeWords, toCut)
fileCorpus <- tm_map(fileCorpus, removeWords, toCut)
tweetCorpus <- tm_map(tweetCorpus, removeWords, toCut)
```
```{r}
dirCorpusDict <- dirCorpus  # create a copy
fileCorpusDict <- fileCorpus  # create a copy
tweetCorpusDict <- tweetCorpus  # create a copy
dirCorpus <- tm_map(dirCorpus, stemDocument)
fileCorpus <- tm_map(fileCorpus, stemDocument)
tweetCorpus <- tm_map(tweetCorpus, stemDocument)
```

```{r}
dirCorpus <- tm_map(dirCorpus, stripWhitespace)
fileCorpus <- tm_map(fileCorpus, stripWhitespace)
tweetCorpus <- tm_map(tweetCorpus, stripWhitespace)
dirCorpusDict <- tm_map(dirCorpusDict, stripWhitespace)
fileCorpusDict <- tm_map(fileCorpusDict, stripWhitespace)
tweetCorpusDict <- tm_map(tweetCorpusDict, stripWhitespace)
```

```{r}
completeStem <- function(x, dictionary) {
    x <- unlist(strsplit(as.character(x), " "))
    x <- x[x != ""]
    x <- stemCompletion(x, dictionary = dictionary, type = "prevalent")
    x <- paste(x, sep = "", collapse = " ")
    PlainTextDocument(stripWhitespace(x))
}

dirCorpus <- lapply(dirCorpus, completeStem, dictionary = dirCorpusDict)
dirCorpus <- Corpus(VectorSource(dirCorpus))
fileCorpus <- lapply(fileCorpus, completeStem, dictionary = fileCorpusDict)
fileCorpus <- Corpus(VectorSource(fileCorpus))
tweetCorpus <- lapply(tweetCorpus, completeStem, dictionary = tweetCorpusDict)
tweetCorpus <- Corpus(VectorSource(tweetCorpus))
```
I let this run for 30 min and it didn't work so I stopped it 

```{r}
dirCorpusDTM <- DocumentTermMatrix(dirCorpus)
fileCorpusDTM <- DocumentTermMatrix(fileCorpus)
tweetCorpusDTM <- DocumentTermMatrix(tweetCorpus)
dirCorpusDTM
```

```{r}
dim(dirCorpusDTM)
```

```{r}
inspect(dirCorpusDTM[1:3, 1:25])  # shows counts in each of the 3 documents of the first 25 words
```
```{r}
fileCorpusDTM
```

```{r}
dim(fileCorpusDTM)
```

```{r}
inspect(fileCorpusDTM[1:7, 1:25])  # shows counts in each of the 7 documents of the first 25 words
```

```{r}
tweetCorpusDTM
```

```{r}
dim(tweetCorpusDTM)
```

```{r}
dirCorpusTDM <- TermDocumentMatrix(dirCorpus)
fileCorpusTDM <- TermDocumentMatrix(fileCorpus)
tweetCorpusTDM <- TermDocumentMatrix(tweetCorpus)
dirCorpusTDM
```

```{r}
dim(dirCorpusTDM)
```

```{r}
inspect(dirCorpusTDM[1:25, 1:3])  # shows counts of the first 25 words in each of the three documents
```

```{r}
fileCorpusTDM
```

```{r}
dim(fileCorpusTDM)
```

```{r}
inspect(fileCorpusTDM[1:25, 1:7])  # shows counts of the first 25 words in each of the seven documents
```

```{r}
tweetCorpusTDM
```

```{r}
dim(tweetCorpusTDM)
```

```{r}
dirCorpusDTM <- removeSparseTerms(dirCorpusDTM, 0.4)  # only terms that appear in at least 40% of the documents will be retained
fileCorpusDTM <- removeSparseTerms(fileCorpusDTM, 0.7)  # only terms that appear in at least 70% of the documents will be retained
dirCorpusTDM <- removeSparseTerms(dirCorpusTDM, 0.4)
fileCorpusTDM <- removeSparseTerms(fileCorpusTDM, 0.7)
inspect(dirCorpusTDM[1:25, 1:3])
```
```{r}
inspect(fileCorpusTDM[1:25, 1:7])
```
#Visualizing Data - Organizing Terms by Frequency
```{r}
dirCorpusFreq <- colSums(as.matrix(dirCorpusDTM))
dirCorpusFreq <- sort(dirCorpusFreq, decreasing = TRUE)
dirCorpusDF <- data.frame(word = names(dirCorpusFreq), freq = dirCorpusFreq)
rownames(dirCorpusDF) <- NULL
head(dirCorpusDF)
```
```{r}
fileCorpusFreq <- colSums(as.matrix(fileCorpusDTM))
fileCorpusFreq <- sort(fileCorpusFreq, decreasing = TRUE)
fileCorpusDF <- data.frame(word = names(fileCorpusFreq), freq = fileCorpusFreq)
rownames(fileCorpusDF) <- NULL
head(fileCorpusDF)
```
```{r}
tweetCorpusFreq <- colSums(as.matrix(tweetCorpusDTM))
tweetCorpusFreq <- sort(tweetCorpusFreq, decreasing = TRUE)
tweetCorpusDF <- data.frame(word = names(tweetCorpusFreq), freq = tweetCorpusFreq)
rownames(tweetCorpusDF) <- NULL
head(tweetCorpusDF)
```
```{r}
# plotting the most common words: Darwin's books
library(ggplot2)
```
```{r}
p <- ggplot(data = dirCorpusDF[1:25, ], aes(x = reorder(word, freq), y = freq)) + 
    xlab("Word") + ylab("Frequency") + geom_bar(stat = "identity") + coord_flip()
p
```

```{r}
# plotting words that occur at least a certain number of times (here, >=
# 1000)
p <- ggplot(subset(dirCorpusDF, freq >= 1000), aes(x = reorder(word, freq), 
    y = freq)) + xlab("Word") + ylab("Frequency") + geom_bar(stat = "identity") + 
    coord_flip()
p
```

```{r}
# plotting the most common words: Austen's novels
p <- ggplot(data = fileCorpusDF[1:25, ], aes(x = reorder(word, freq), y = freq)) + 
    xlab("Word") + ylab("Frequency") + geom_bar(stat = "identity") + coord_flip()
p
```

```{r}
# plotting words that occur at least a certain number of times (here, >=
# 1000)
p <- ggplot(subset(fileCorpusDF, freq >= 1000), aes(x = reorder(word, freq), 
    y = freq)) + xlab("Word") + ylab("Frequency") + geom_bar(stat = "identity") + 
    coord_flip()
p
```

```{r}
# an alternative way to find a list of common words and print as a vector
findFreqTerms(fileCorpusDTM, lowfreq = 1000)
```
```{r}
# we can also find the correlations between words, a measure of how often
# they co-occur across documents
findAssocs(fileCorpusDTM, terms = c("pride", "anger"), corlimit = 0.9)
```
#Visualizing Data - Plotting Correlations and Clustering Among Terms and Among Documents
```{r}
library(Rgraphviz)
```
```{r}
attrs = list(node = list(fillcolor = "yellow", fontsize = "30"), edge = list(), 
    graph = list())
plot(tweetCorpusDTM, terms = findFreqTerms(tweetCorpusDTM, lowfreq = 11), attrs = attrs, 
    corThreshold = 0.1)
```
```{r, eval=FALSE}
dev.off()  # clears the plot window for next plot
```
no idea why I got this error

```{r}
library(cluster)
fileDocDist <- dist(scale(fileCorpusDTM), method = "euclidian")
fitDoc <- hclust(fileDocDist, method = "ward.D2")
library(dendextend)
```
```{r}
dend <- as.dendrogram(fitDoc)  # similarity among DOCUMENTS
dend <- rotate(dend, 1:length(fitDoc$labels))
dend <- color_branches(dend, k = 3)
dend <- set(dend, "labels_cex", 1)
dend <- hang.dendrogram(dend, hang_height = 0.1)
plot(dend, horiz = TRUE, main = "Similarity among Jane Austen Novels in Term Use")
```
```{r, eval= FALSE}
dev.off()  # clears the plot window for next plot
```
```{r}
tweetCorpusTDM <- TermDocumentMatrix(tweetCorpus, control = list(bounds = list(global = c(11, 
    Inf))))
tweetTermDist <- dist(scale(tweetCorpusTDM), method = "euclidian")
fitTerm <- hclust(tweetTermDist, method = "ward.D2")
dend <- as.dendrogram(fitTerm)  # similarity among TERMS
dend <- rotate(dend, 1:length(fitTerm$labels))
dend <- color_branches(dend, k = 5)
dend <- set(dend, "labels_cex", 1)
dend <- hang.dendrogram(dend, hang_height = 1)
plot(dend, horiz = TRUE, main = "Similarity in Term Use Across Obama Tweets")
```
```{r, eval=FALSE}
dev.off()  # clears the plot window for next plot
```
```{r}
library(wordcloud)
```
```{r}
# for Darwin's books
set.seed(1)
wordcloud(dirCorpusDF$word, dirCorpusDF$freq, min.freq = 500)
```
```{r}
set.seed(1)
wordcloud(dirCorpusDF$word, dirCorpusDF$freq, max.words = 100, rot.per = 0.2, 
    colors = brewer.pal(6, "Accent"))
```
```{r}
# for Austen's novels
set.seed(1)
wordcloud(fileCorpusDF$word, fileCorpusDF$freq, min.freq = 500)
```
```{r}
set.seed(1)
wordcloud(fileCorpusDF$word, fileCorpusDF$freq, max.words = 100, rot.per = 0.2, 
    colors = brewer.pal(6, "Accent"))
```

```{r}
# for Obama's tweets
set.seed(1)
wordcloud(tweetCorpusDF$word, tweetCorpusDF$freq, min.freq = 500)
```


































































